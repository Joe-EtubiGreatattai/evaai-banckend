const { sendMessage } = require('./chatController');
const User = require('../models/User');
const fs = require('fs');
const path = require('path');
const { OpenAI } = require('openai');
const { MessageMedia } = require('whatsapp-web.js');
const { generateInvoicePDF } = require('./../services/actionHandlers/invoiceActions');

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

async function handleWhatsAppMessage(msg) {
  console.log('ğŸ“© New incoming WhatsApp message:', {
    from: msg.from,
    body: msg.body,
    hasMedia: msg.hasMedia,
  });

  try {
    const senderId = msg.from;
    const rawPhone = senderId.split('@')[0];

    let user = await User.findOne({ phoneNumber: rawPhone });

    if (!user) {
      console.log(`ğŸ‘¤ No user found for ${rawPhone}. Creating one...`);
      user = await User.create({
        phoneNumber: rawPhone,
        fullName: 'WhatsApp User',
        email: `whatsapp.${rawPhone}@eveai.ai`,
        isWhatsAppUser: true,
        whatsappProfileName: 'WhatsApp User'
      });
      console.log(`âœ… User created with ID: ${user._id}`);
    }

    let messageText = msg.body || '';
    let userUsedVoiceNote = false;

    if (msg.hasMedia) {
      console.log('ğŸ“· Media message detected. Attempting to download...');
      const media = await msg.downloadMedia();
      if (!media) return msg.reply("Couldn't download the media, please try again.");

      const rawExt = media.mimetype.split('/')[1];
      const ext = rawExt.split(';')[0];
      const filename = `media_${Date.now()}.${ext}`;
      const downloadsDir = path.join(__dirname, 'downloads');

      if (!fs.existsSync(downloadsDir)) fs.mkdirSync(downloadsDir, { recursive: true });

      const filePath = path.join(downloadsDir, filename);
      fs.writeFileSync(filePath, Buffer.from(media.data, 'base64'));
      console.log(`âœ… Media saved: ${filePath}`);

      const isImage = media.mimetype.startsWith('image');
      const isAudio = media.mimetype.startsWith('audio');

      if (isImage) {
        console.log('ğŸ–¼ï¸ Processing image with AI...');
        try {
          const caption = msg.body || "Describe this image";

          const gptResponse = await openai.chat.completions.create({
            model: 'gpt-4o',
            messages: [
              {
                role: "system",
                content: "You are an AI assistant that describes images in detail. Provide a comprehensive description of what you see in the image."
              },
              {
                role: "user",
                content: [
                  { type: "text", text: `Please describe this image in detail. User's caption/question: "${caption}"` },
                  { type: "image_url", image_url: { url: `data:${media.mimetype};base64,${media.data}` } }
                ],
              },
            ],
            max_tokens: 500,
          });

          const imageDescription = gptResponse.choices[0].message.content.trim();
          console.log('ğŸ–¼ï¸ Image description generated:', imageDescription);
          messageText = `Image: ${imageDescription}`;
          fs.unlinkSync(filePath);

        } catch (error) {
          console.error('âŒ Error processing image:', error);
          messageText = `Image: Unable to process the image.`;
        }

      } else if (isAudio) {
        userUsedVoiceNote = true;
        console.log('ğŸ™ï¸ Transcribing audio...');
        try {
          const transcription = await openai.audio.transcriptions.create({
            file: fs.createReadStream(filePath),
            model: "whisper-1",
          });

          const transcribedText = transcription.text.trim();
          console.log('ğŸ“ Audio transcription:', transcribedText);
          messageText = transcribedText;
          fs.unlinkSync(filePath);

        } catch (error) {
          console.error('âŒ Audio transcription failed:', error);
          messageText = `Voice message: Unable to transcribe the audio.`;
        }

      } else {
        console.log(`ğŸ“ Unsupported media type: ${media.mimetype}`);
        messageText = `Unsupported media type: ${media.mimetype}`;
        fs.unlinkSync(filePath);
      }
    }

    if (!messageText.trim()) {
      return msg.reply('Please send a valid text or voice message.');
    }

    console.log('ğŸ§  Forwarding message to AI:', messageText);

    const mockReq = {
      user: { id: user._id },
      body: { text: messageText }
    };

    const mockRes = {
      status: () => mockRes,
      json: async (response) => {
        try {
          const aiResponse = response.data.messages.find(m => m.sender === 'assistant')?.text || 'No response generated.';
          console.log('ğŸ¤– AI response:', aiResponse);

          const createdInvoice = response.data?.actionResult?.data;
          const isInvoiceCreated = response.data?.actionResult?.success &&
                                   createdInvoice?.clientName &&
                                   createdInvoice?.amount &&
                                   createdInvoice?.date;

          if (isInvoiceCreated) {
            const pdfBuffer = await generateInvoicePDF(createdInvoice);
            const base64PDF = pdfBuffer.toString('base64');
            const media = new MessageMedia("application/pdf", base64PDF, `invoice-${createdInvoice.invoiceNumber || createdInvoice._id}.pdf`);

            if (userUsedVoiceNote) {
              const tts = await openai.audio.speech.create({
                model: "tts-1",
                input: aiResponse,
                voice: "nova",
                response_format: "mp3"
              });

              const audioBuffer = Buffer.from(await tts.arrayBuffer());
              const audioMedia = new MessageMedia("audio/mpeg", audioBuffer.toString("base64"));
              await msg.reply(audioMedia);
            } else {
              await msg.reply(aiResponse);
            }

            await msg.reply(media);
            return;
          }

          if (userUsedVoiceNote) {
            const tts = await openai.audio.speech.create({
              model: "tts-1",
              input: aiResponse,
              voice: "nova",
              response_format: "mp3"
            });

            const audioBuffer = Buffer.from(await tts.arrayBuffer());
            const audioMedia = new MessageMedia("audio/mpeg", audioBuffer.toString("base64"));
            await msg.reply(audioMedia);
          } else {
            await msg.reply(aiResponse);
          }

        } catch (err) {
          console.error("âŒ Error handling AI response:", err);
          msg.reply("Something went wrong while generating a response.");
        }
      }
    };

    await sendMessage(mockReq, mockRes, (err) => {
      if (err) {
        console.error('âŒ sendMessage error:', err);
        msg.reply("Something went wrong while processing your message.");
      }
    });

  } catch (error) {
    console.error('ğŸ’¥ Fatal error in message handler:', error);
    msg.reply('Something went wrong on our end.');
  }
}

module.exports = { handleWhatsAppMessage };
